{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and Variance\n",
    "================\n",
    "\n",
    "Original source: \n",
    "[*Machine learning class*](https://sites.google.com/a/unal.edu.co/machine-learning-2013-2/) *support material, Universidad Nacional de Colombia, 2013*\n",
    "\n",
    "The purpose of this notebook is to illustrate the bias-variance trade-off when learning regression models from data. We will use and example based on non-linear regression.\n",
    "\n",
    "Training data generation\n",
    "------------------------\n",
    "\n",
    "First we will write a function to generate a random sample. The data generation model is the following:\n",
    "\n",
    "$$r(x) = f(x) + \\epsilon$$\n",
    "\n",
    "with $\\epsilon\\sim\\mathcal{N}(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(size):\n",
    "    '''\n",
    "    Returns a sample with 'size' instances without noise.\n",
    "    '''\n",
    "    x = np.linspace(0, 4.5, size)\n",
    "    y = 2 * np.sin(x * 1.5)\n",
    "    return (x,y)\n",
    "\n",
    "def sample(size):\n",
    "    '''\n",
    "    Returns a sample with 'size' instances.\n",
    "    '''\n",
    "    #x = np.sort(np.random.uniform(0, 4.5, size))\n",
    "    x = np.linspace(0, 4.5, size)\n",
    "    y = 2 * np.sin(x * 1.5) + np.random.randn(x.size)\n",
    "    return (x,y)\n",
    "    \n",
    "plt.figure()\n",
    "f_x, f_y = f(50) # 50 ideal samples\n",
    "plt.plot(f_x, f_y) # Plot them with a line\n",
    "x, y = sample(50) # 50 noisy samples\n",
    "plt.plot(x, y, 'k.') # plot them as dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model fitting\n",
    "=============\n",
    "\n",
    "We will use least square regression (LSR) to fit a polynomial to the data. Actually, we will use multivariate linear regression, over a dataset built in the following way:\n",
    "\n",
    "For each sample $x_{i}$ we build a vector $(1 , x_{i} , x_{i}^{2} , \\dots , x_{i}^{n})$  and we use LSR to fit a function $g:\\mathbb{R}^{n+1}\\rightarrow\\mathbb{R}$ to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This illustrates how vander function works:\n",
    "x1 = np.array([1,2,3])\n",
    "print(np.vander(x1, 4)) # x^3 x^2 x^1 x^0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def fit_polynomial(x, y, degree):\n",
    "    '''\n",
    "    Fits a polynomial to the input sample.\n",
    "    (x,y): input sample\n",
    "    degree: polynomial degree\n",
    "    '''\n",
    "    model = LinearRegression()\n",
    "    model.fit(np.vander(x, degree + 1), y)\n",
    "    return model\n",
    "\n",
    "def apply_polynomial(model, x):\n",
    "    '''\n",
    "    Evaluates a linear regression model in an input sample\n",
    "    model: linear regression model\n",
    "    x: input sample\n",
    "    '''\n",
    "    degree = model.coef_.size - 1\n",
    "    y = model.predict(np.vander(x, degree + 1))\n",
    "    return y\n",
    "\n",
    "model = fit_polynomial(x, y, 8)\n",
    "p_y = apply_polynomial(model, x)\n",
    "plt.plot(f_x, f_y,label='Ideal samples')\n",
    "plt.plot(x, y, 'k.',label='Training')\n",
    "plt.plot(x, p_y,label='Predicted')\n",
    "\n",
    "legend = plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model averaging\n",
    "---------------\n",
    "\n",
    "The following code generates a set of samples of the same size and fits a poynomial to each sample. Then the average model is calculated. All the models, including the average model, are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 15\n",
    "n_samples = 30\n",
    "n_models = 100\n",
    "avg_y = np.zeros(n_samples)\n",
    "\n",
    "plt.figure()\n",
    "for i in np.arange(n_models):\n",
    "    (x,y) = sample(n_samples)\n",
    "    model = fit_polynomial(x, y, degree)\n",
    "    p_y = apply_polynomial(model, x)\n",
    "    avg_y = avg_y + p_y\n",
    "    plt.plot(x, p_y, '-',alpha=0.3)\n",
    "avg_y = avg_y / n_models\n",
    "plt.plot(x, avg_y, 'k--',linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating bias and variance\n",
    "-----------------------------\n",
    "\n",
    "Same as previous example, we generate several samples and fit a polynomial to each one. We calculate bias an variance among models for different polynomial degrees. Bias, variance and error are plotted against different degree values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "n_samples = 20\n",
    "f_x, f_y = f(n_samples)\n",
    "n_models = 100\n",
    "max_degree = 15\n",
    "var_vals =[]\n",
    "bias_vals = []\n",
    "error_vals = []\n",
    "for degree in np.arange(1, max_degree):\n",
    "    avg_y = np.zeros(n_samples)\n",
    "    models = []\n",
    "    for i in np.arange(n_models):\n",
    "        (x,y) = sample(n_samples)\n",
    "        model = fit_polynomial(x, y, degree)\n",
    "        p_y = apply_polynomial(model, x)\n",
    "        avg_y = avg_y + p_y\n",
    "        models.append(p_y)\n",
    "        \n",
    "    # Compute the average of all estimations    \n",
    "    avg_y = avg_y / n_models\n",
    "    \n",
    "    # Compute the distance L2 between the average and the ideal as bias estimation\n",
    "    bias_2 = norm(avg_y - f_y)/f_y.size\n",
    "    bias_vals.append(bias_2)\n",
    "    variance = 0\n",
    "    \n",
    "    # Compute now the variance among all estimations\n",
    "    for p_y in models:\n",
    "        variance += norm(avg_y - p_y)\n",
    "    variance /= f_y.size * n_models\n",
    "    var_vals.append(variance)\n",
    "    error_vals.append(variance + bias_2)\n",
    "    \n",
    "plt.figure()    \n",
    "plt.plot(range(1, max_degree), bias_vals, label='bias')\n",
    "plt.plot(range(1, max_degree), var_vals, label='variance')\n",
    "plt.plot(range(1, max_degree), error_vals, label='error')\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation\n",
    "----------------\n",
    "\n",
    "Since in a real setup we don't have access to the real $f$ function. We cannot exactly calculate the error; however, we can approximate it using cross validation. We generate two sets: a training set and a validation set. The validation set is used to calculate an estimation of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "# train sample\n",
    "train_x, train_y = sample(n_samples)\n",
    "# validation sample\n",
    "test_x, test_y = sample(n_samples)\n",
    "max_degree = 20\n",
    "test_error_vals = []\n",
    "train_error_vals = []\n",
    "for degree in np.arange(1, max_degree):\n",
    "    model = fit_polynomial(train_x, train_y, degree)\n",
    "    p_y = apply_polynomial(model, train_x)\n",
    "    train_error_vals.append(np.linalg.norm(train_y - p_y)**2)\n",
    "    p_y = apply_polynomial(model, test_x)\n",
    "    test_error_vals.append(np.linalg.norm(test_y - p_y)**2)\n",
    "plt.figure()\n",
    "plt.plot(range(1, max_degree), test_error_vals, label='test error')\n",
    "plt.plot(range(1, max_degree), train_error_vals, label='train error')\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization\n",
    "--------------\n",
    "\n",
    "Another way to deal with the model complexity is using regularization. A regularizer is a term that penalizes the model complexity and is part of the loss function. the next portion of code shows how the norm of the coefficients of the linear regression model increased when the complexity of the model (polynomial degree) increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "train_x, train_y = sample(n_samples)\n",
    "max_degree = 9\n",
    "w_norm = []\n",
    "for degree in np.arange(1, max_degree):\n",
    "    model = fit_polynomial(train_x, train_y, degree)\n",
    "    w_norm.append(np.linalg.norm(model.coef_))\n",
    "plt.plot(range(1, max_degree), w_norm, label='||w||')\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result suggests that we can control the complexity by penalizing the norm of the model's weights, $||w||$. This idea is implemented by the *Ridge Regression* method.\n",
    "\n",
    "### Ridge regression ###\n",
    "\n",
    "Ridge regression finds a regression model by minimizing the following loss function:\n",
    "\n",
    "$$ \\min_{W}\\left\\Vert WX-Y\\right\\Vert ^{2}+\\alpha\\left\\Vert W\\right\\Vert ^{2} $$\n",
    "\n",
    "where $X$ and $Y$ are the input matrix and the output vector respectively. The parameter $\\alpha$ controls the amount of regularization. You can find more information in the documentation of [scikit-learn ridge regresion implementation](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression). \n",
    "\n",
    "\n",
    "#### Exercise ####\n",
    "\n",
    "Repeat the cross validation experiment using ridge regression. Use a fixed polynomial degree (e.g. 10) and vary the $\\alpha$ parameter.\n",
    "\n",
    "___________________\n",
    "\n",
    "<h2 id=\"biblio\"> References </h2>\n",
    "\n",
    "* [Alpaydin10] Alpaydin, E. 2010 [Introduction to Machine Learning][alp10], 2Ed. The MIT Press.\n",
    "\n",
    "[alp10]: http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
