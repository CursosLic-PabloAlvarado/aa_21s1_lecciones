{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Based on: https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "\n",
    "This notebook uses Taxi-v3 (v2 in original code is deprecated).  \n",
    "\n",
    "For more information, check gym's site:\n",
    "\n",
    "https://gym.openai.com/envs/Taxi-v3/\n",
    "\n",
    "The Taxi Problem <br />\n",
    "from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"<br />\n",
    "by Tom Dietterich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment\n",
    "\n",
    "Rendering:\n",
    "* The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "* The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "* R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination.\n",
    "\n",
    "We have an Action Space of size 6 and a State Space of size 500. \n",
    "\n",
    "The actions are:\n",
    "\n",
    "* 0 = south\n",
    "* 1 = north\n",
    "* 2 = east\n",
    "* 3 = west\n",
    "* 4 = pickup\n",
    "* 5 = dropoff\n",
    "\n",
    "There are 500 discrete states since there are \n",
    "* 25 taxi positions,\n",
    "* 5 possible locations of the passenger (including the case when the passenger is in the taxi), and\n",
    "* 4 destination locations. \n",
    "\n",
    "Passenger locations:\n",
    "* 0: R(ed)\n",
    "* 1: G(reen)\n",
    "* 2: Y(ellow)\n",
    "* 3: B(lue)\n",
    "* 4: in taxi\n",
    "\n",
    "Destinations:\n",
    "* 0: R(ed)\n",
    "* 1: G(reen)\n",
    "* 2: Y(ellow)\n",
    "* 3: B(lue)\n",
    "\n",
    "state space is represented by:\n",
    "* (taxi_row, taxi_col, passenger_location, destination)\n",
    "\n",
    "Rewards:\n",
    "* There is a default per-step reward of -1,\n",
    "* except for delivering the passenger, which is +20,\n",
    "* or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
    "\n",
    "The optimal action for each state is the action that has the highest cumulative long-term reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our current state? (row,column,passenger index,destination index)\n",
    "\n",
    "env.render()\n",
    "print(\"State:\",env.s,list(env.decode(env.s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The reward table\n",
    "\n",
    "Here we have a reward dependent on the current state and action.  The reward table called `P` is a dictionary with the number of states as rows and number of actions as columns, i.e. a states Ã— actions matrix.\n",
    "\n",
    "This dictionary has the structure {action: [(probability, nextstate, reward, done)]}.\n",
    "\n",
    "Since every state is in this matrix, we can see the default reward values assigned to our current state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.P[env.s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving with random policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have our P table for default rewards in each state, we can try to have our taxi navigate just using that.\n",
    "\n",
    "We'll create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 20. The env.action_space.sample() method automatically selects one random action from set of all possible actions.\n",
    "\n",
    "Let's see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames[0:100]):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.04)\n",
    "        \n",
    "#print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q-Table of states x actions\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "## Episode iteration\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        ## Epsilon-Greedy\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")\n",
    "print(\"Penalties:\",penalties, \"Epochs:\",epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Q-table has been established over 100,000 episodes, let's see what the Q-values are at our illustration's state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.s = 328 # force state\n",
    "env.render()\n",
    "\n",
    "print(q_table[328])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        # print(\"Action:\",action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        # print(\"Reward: {0}  done: {1}  info: {2}\".format(reward,done,info))\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "\n",
    "## Let's plot the agent in action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
