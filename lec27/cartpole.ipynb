{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole - DQN\n",
    "\n",
    "Original environment: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py<br/>\n",
    "Source:  https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288\n",
    "\n",
    "###  Observation:\n",
    "\n",
    "Type: Box(4)\n",
    "\n",
    "| Num | Observation | Min | Max |\n",
    "|:---:|:-----------:|:---:|:---:|\n",
    "| 0   | Cart Position|-4.8|4.8|\n",
    "| 1   | Cart Velocity|-Inf|Inf|\n",
    "| 2   | Pole Angle   | -0.418 rad (-24 deg)|0.418 rad (24 deg)|\n",
    "| 3   | Pole Angular Velocity|-Inf|Inf|\n",
    "\n",
    "### Actions:\n",
    "\n",
    "Type: Discrete(2)\n",
    "\n",
    "| Num | Action                  |\n",
    "|:---:|:-----------------------:|\n",
    "|  0  |  Push cart to the left  |\n",
    "|  1  |  Push cart to the right |\n",
    "\n",
    "Note: The amount the velocity that is reduced or increased is not\n",
    "fixed; it depends on the angle the pole is pointing. This is because\n",
    "the center of gravity of the pole increases the amount of energy needed\n",
    "to move the cart underneath it\n",
    "\n",
    "### Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "## Suppress TensorFlow Info and Warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "import gym\n",
    "import random\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "## With TF2 we might need this or otherwise it will be too slow\n",
    "## In some hardware and library version configurations, it might\n",
    "## be exactly the opposite.\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from math import exp,cos\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import seed,randn\n",
    "\n",
    "from collections import deque\n",
    "from statistics import mean\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "MAX_MEMORY = 100000\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.975\n",
    "EXPLORATION_DECAY = 0.99\n",
    "EXPLORATION_MIN = 0.01\n",
    "EPISODES=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MAX_MEMORY)\n",
    "        self.exploration_rate = 1.0\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(32, input_shape=(observation_space,), activation='relu'))\n",
    "        self.model.add(Dense(32, activation='relu'))\n",
    "        self.model.add(Dense(self.action_space, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(0, self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        else:\n",
    "            minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "            ## TODO:\n",
    "            ## This loop trains one sample at a time the model, but we could\n",
    "            ## use the whole minibatch at once\n",
    "\n",
    "            for state, action, reward, state_next, done in minibatch:\n",
    "                Q = reward\n",
    "                if not done:\n",
    "                    Q = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "                Q_values = self.model.predict(state)\n",
    "                Q_values[0][action] = Q\n",
    "                self.model.fit(state, Q_values, verbose=0)\n",
    "            self.exploration_rate *= EXPLORATION_DECAY\n",
    "            self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def load_model(self,model_name):\n",
    "        self.model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSolver:\n",
    "\n",
    "    def __init__(self, max_episodes):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.score_table = deque(maxlen=400)\n",
    "        self.average_of_last_runs = None\n",
    "        self.model = None\n",
    "        self.play_episodes = 100\n",
    "        env = gym.make('CartPole-v1')\n",
    "        observation_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        self.solver = Network(observation_space, action_space)\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        env = gym.make('CartPole-v1')\n",
    "        observation_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "\n",
    "        print(\"---------------------------------\")\n",
    "        print(\"Solver starts\")\n",
    "        print(\"---------------------------------\")\n",
    "\n",
    "        self.model = self.solver.get_model()\n",
    "           \n",
    "        episode = 0\n",
    "        while episode < self.max_episodes:\n",
    "\n",
    "            episode += 1\n",
    "            state = env.reset()\n",
    "\n",
    "            ## Hack a more diverse initial random position\n",
    "            x, x_dot, theta, theta_dot = env.state\n",
    "            x = randn()*3;\n",
    "            env.state = (x,x_dot,theta,theta_dot)\n",
    "\n",
    "            state = np.reshape(np.array(env.state), [1, observation_space])\n",
    "            \n",
    "            step = 0\n",
    "            while True:\n",
    "\n",
    "                env.render()\n",
    "                \n",
    "                step += 1\n",
    "                action = self.solver.take_action(state)\n",
    "                state_next, reward, done, info = env.step(action)\n",
    "\n",
    "                state_next = np.reshape(state_next, [1, observation_space])\n",
    "\n",
    "                ## State is a vector with one observation\n",
    "                ## Type: Box(4)\n",
    "                ## Num  Observation                 Min         Max\n",
    "                ## 0    Cart Position             -4.8            4.8\n",
    "                ## 1    Cart Velocity             -Inf            Inf\n",
    "                ## 2    Pole Angle                 -24 deg        24 deg\n",
    "                ## 3    Pole Velocity At Tip      -Inf            Inf\n",
    "                \n",
    "                ## Prefer to be in the middle and vertical\n",
    "                reward = exp(-0.5*abs((state_next[0][0]**2)/0.5)) * \\\n",
    "                         cos(state_next[0][2])\n",
    "                \n",
    "                ##if not done:\n",
    "                ##    reward = reward\n",
    "                ##else:\n",
    "                ##    reward = exp(-0.5*abs((state_next[0][0]**2)/0.5)) - 0.2\n",
    "\n",
    "                self.solver.add_to_memory(state, action, reward, state_next, done)\n",
    "                state = state_next\n",
    "\n",
    "                # print(\"  State: \" + str(state) +\n",
    "                #       \", reward: \" + str(reward) +\n",
    "                #       \"               \",\n",
    "                #       end='\\r', flush=True)\n",
    "                \n",
    "                if done:\n",
    "                    print(\"Run: \" + str(episode) +\n",
    "                          \", exploration: \"+str(self.solver.exploration_rate) +\n",
    "                          \", score: \" + str(step) +\n",
    "                          \", mem: \" + str(len(self.solver.memory)))\n",
    "\n",
    "                    break\n",
    "                ## Train the network\n",
    "                self.solver.experience_replay()\n",
    "\n",
    "    def return_trained_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.save('cartpole_model.h5')\n",
    "\n",
    "    def load_model(self):\n",
    "        filename = 'cartpole_model.h5'\n",
    "        if os.path.exists(filename):\n",
    "            self.solver.load_model(filename)\n",
    "            self.model = self.solver.get_model()\n",
    "        else:\n",
    "            print(\"File '\" + filename + \"' does not exist. Ignoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Solver starts\n",
      "---------------------------------\n",
      "\n",
      "Run: 1, exploration: 1.0, score: 18, mem: 18\n",
      "\n",
      "Run: 2, exploration: 1.0, score: 13, mem: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/palvarado/anaconda3/envs/aa/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run: 3, exploration: 0.8775210229989678, score: 14, mem: 45\n",
      "\n",
      "Run: 4, exploration: 0.7249803359578534, score: 20, mem: 65\n",
      "\n",
      "Run: 5, exploration: 0.5416850759668536, score: 30, mem: 95\n",
      "\n",
      "Run: 6, exploration: 0.4801414565714212, score: 13, mem: 108\n",
      "\n",
      "Run: 7, exploration: 0.4386175018099108, score: 10, mem: 118\n",
      "\n",
      "Run: 8, exploration: 0.392711028357805, score: 12, mem: 130\n",
      "\n",
      "Run: 9, exploration: 0.35516081470507305, score: 11, mem: 141\n",
      "\n",
      "Run: 10, exploration: 0.22594815553398728, score: 46, mem: 187\n",
      "\n",
      "Run: 11, exploration: 0.21059844619672854, score: 8, mem: 195\n",
      "\n",
      "Run: 12, exploration: 0.18855684516737714, score: 12, mem: 207\n",
      "\n",
      "Run: 13, exploration: 0.17224993019150142, score: 10, mem: 217\n",
      "\n",
      "Run: 14, exploration: 0.15422195179384465, score: 12, mem: 229\n",
      "\n",
      "Run: 15, exploration: 0.14374493715362485, score: 8, mem: 237\n",
      "\n",
      "Run: 16, exploration: 0.13000034453500542, score: 11, mem: 248\n",
      "\n",
      "Run: 17, exploration: 0.11875755691154315, score: 10, mem: 258\n",
      "\n",
      "Run: 18, exploration: 0.10632818368521123, score: 12, mem: 270\n",
      "\n",
      "Run: 19, exploration: 0.08784500919014843, score: 20, mem: 290\n",
      "\n",
      "Run: 20, exploration: 0.06497898609824969, score: 31, mem: 321\n",
      "\n",
      "Run: 21, exploration: 0.03853035847745153, score: 53, mem: 374\n",
      "\n",
      "Run: 22, exploration: 0.023784410415102923, score: 49, mem: 423\n",
      "\n",
      "Run: 23, exploration: 0.01741740389222302, score: 32, mem: 455\n",
      "\n",
      "Run: 24, exploration: 0.012129710294652202, score: 37, mem: 492\n",
      "\n",
      "Run: 25, exploration: 0.01, score: 35, mem: 527\n",
      "\n",
      "Run: 26, exploration: 0.01, score: 33, mem: 560\n",
      "\n",
      "Run: 27, exploration: 0.01, score: 34, mem: 594\n",
      "\n",
      "Run: 28, exploration: 0.01, score: 26, mem: 620\n",
      "\n",
      "Run: 29, exploration: 0.01, score: 44, mem: 664\n",
      "\n",
      "Run: 30, exploration: 0.01, score: 91, mem: 755\n",
      "\n",
      "Run: 31, exploration: 0.01, score: 193, mem: 948\n",
      "\n",
      "Run: 32, exploration: 0.01, score: 134, mem: 1082\n",
      "\n",
      "Run: 33, exploration: 0.01, score: 154, mem: 1236\n",
      "\n",
      "Run: 34, exploration: 0.01, score: 131, mem: 1367\n",
      "\n",
      "Run: 35, exploration: 0.01, score: 146, mem: 1513\n",
      "\n",
      "Run: 36, exploration: 0.01, score: 130, mem: 1643\n",
      "\n",
      "Run: 37, exploration: 0.01, score: 157, mem: 1800\n",
      "\n",
      "Run: 38, exploration: 0.01, score: 179, mem: 1979\n",
      "\n",
      "Run: 39, exploration: 0.01, score: 218, mem: 2197\n",
      "\n",
      "Run: 40, exploration: 0.01, score: 214, mem: 2411\n",
      "\n",
      "Run: 41, exploration: 0.01, score: 217, mem: 2628\n",
      "\n",
      "Run: 42, exploration: 0.01, score: 221, mem: 2849\n",
      "\n",
      "Run: 43, exploration: 0.01, score: 195, mem: 3044\n",
      "\n",
      "Run: 44, exploration: 0.01, score: 200, mem: 3244\n",
      "\n",
      "Run: 45, exploration: 0.01, score: 211, mem: 3455\n",
      "\n",
      "Run: 46, exploration: 0.01, score: 291, mem: 3746\n",
      "\n",
      "Run: 47, exploration: 0.01, score: 298, mem: 4044\n",
      "\n",
      "Run: 48, exploration: 0.01, score: 216, mem: 4260\n",
      "\n",
      "Run: 49, exploration: 0.01, score: 305, mem: 4565\n",
      "\n",
      "Run: 50, exploration: 0.01, score: 387, mem: 4952\n",
      "\n",
      "Run: 51, exploration: 0.01, score: 388, mem: 5340\n",
      "\n",
      "Run: 52, exploration: 0.01, score: 500, mem: 5840\n",
      "\n",
      "Run: 53, exploration: 0.01, score: 297, mem: 6137\n",
      "\n",
      "Run: 54, exploration: 0.01, score: 283, mem: 6420\n",
      "\n",
      "Run: 55, exploration: 0.01, score: 449, mem: 6869\n",
      "\n",
      "Run: 56, exploration: 0.01, score: 500, mem: 7369\n",
      "\n",
      "Run: 57, exploration: 0.01, score: 500, mem: 7869\n",
      "\n",
      "Run: 58, exploration: 0.01, score: 175, mem: 8044\n",
      "\n",
      "Run: 59, exploration: 0.01, score: 452, mem: 8496\n",
      "\n",
      "Run: 60, exploration: 0.01, score: 500, mem: 8996\n",
      "\n",
      "Run: 61, exploration: 0.01, score: 295, mem: 9291\n",
      "\n",
      "Run: 62, exploration: 0.01, score: 500, mem: 9791\n",
      "\n",
      "Run: 63, exploration: 0.01, score: 500, mem: 10291\n",
      "\n",
      "Run: 64, exploration: 0.01, score: 362, mem: 10653\n",
      "\n",
      "Run: 65, exploration: 0.01, score: 500, mem: 11153\n",
      "\n",
      "Run: 66, exploration: 0.01, score: 500, mem: 11653\n",
      "\n",
      "Run: 67, exploration: 0.01, score: 459, mem: 12112\n",
      "\n",
      "Run: 68, exploration: 0.01, score: 137, mem: 12249\n",
      "\n",
      "Run: 69, exploration: 0.01, score: 10, mem: 12259\n",
      "\n",
      "Run: 70, exploration: 0.01, score: 9, mem: 12268\n",
      "\n",
      "Run: 71, exploration: 0.01, score: 8, mem: 12276\n",
      "\n",
      "Run: 72, exploration: 0.01, score: 79, mem: 12355\n",
      "\n",
      "Run: 73, exploration: 0.01, score: 25, mem: 12380\n",
      "\n",
      "Run: 74, exploration: 0.01, score: 37, mem: 12417\n",
      "\n",
      "Run: 75, exploration: 0.01, score: 31, mem: 12448\n",
      "\n",
      "Run: 76, exploration: 0.01, score: 11, mem: 12459\n",
      "\n",
      "Run: 77, exploration: 0.01, score: 17, mem: 12476\n",
      "\n",
      "Run: 78, exploration: 0.01, score: 27, mem: 12503\n",
      "\n",
      "Run: 79, exploration: 0.01, score: 27, mem: 12530\n",
      "\n",
      "Run: 80, exploration: 0.01, score: 32, mem: 12562\n",
      "\n",
      "Run: 81, exploration: 0.01, score: 500, mem: 13062\n",
      "\n",
      "Run: 82, exploration: 0.01, score: 500, mem: 13562\n",
      "\n",
      "Run: 83, exploration: 0.01, score: 500, mem: 14062\n",
      "\n",
      "Run: 84, exploration: 0.01, score: 500, mem: 14562\n",
      "\n",
      "Run: 85, exploration: 0.01, score: 500, mem: 15062\n",
      "\n",
      "Run: 86, exploration: 0.01, score: 489, mem: 15551\n",
      "\n",
      "Run: 87, exploration: 0.01, score: 500, mem: 16051\n",
      "\n",
      "Run: 88, exploration: 0.01, score: 50, mem: 16101\n",
      "\n",
      "Run: 89, exploration: 0.01, score: 500, mem: 16601\n",
      "\n",
      "Run: 90, exploration: 0.01, score: 465, mem: 17066\n",
      "\n",
      "Run: 91, exploration: 0.01, score: 500, mem: 17566\n",
      "\n",
      "Run: 92, exploration: 0.01, score: 500, mem: 18066\n",
      "\n",
      "Run: 93, exploration: 0.01, score: 500, mem: 18566\n",
      "\n",
      "Run: 94, exploration: 0.01, score: 500, mem: 19066\n",
      "\n",
      "Run: 95, exploration: 0.01, score: 500, mem: 19566\n",
      "\n",
      "Run: 96, exploration: 0.01, score: 500, mem: 20066\n"
     ]
    }
   ],
   "source": [
    "RL=TrainSolver(EPISODES)\n",
    "RL.load_model();\n",
    "RL.train()\n",
    "RL.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
