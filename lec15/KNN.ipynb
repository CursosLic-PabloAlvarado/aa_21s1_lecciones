{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-parametric classification: \n",
    "## Using Kernel Density Estimation and K Nearest Neighbours \n",
    "---\n",
    "\n",
    "Source: https://icaml.org/canon/algo/bayesian-classification/KDE_KNN/KNN.html\n",
    "\n",
    "__Author__: Dennis Wittich\n",
    "\n",
    "__Date__: 06 / 05 / 2019\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we implement two non-parametric classification approaches: __Kernel Density Estimation (KDE)__ and __K Nearest Neighbours (KNN)__. Both approaches are based on the assumption that the probability of a sample $x$ for belonging to class $k$ can be approximated as\n",
    "\n",
    "$p(x \\mid  C=L^k)\\approx \\dfrac{K_k}{N_k \\cdot V}$\n",
    "\n",
    "where\n",
    "\n",
    "- $V$ is a volume around $x$ \n",
    "- $N_k$ is the number of training samples of class $k$\n",
    "- $K_k$ is the number of training samples of class $k$ that fall into $V$\n",
    "\n",
    "In order to evaluate $K_k$, all training samples have to be accessible, which is a drawback of these approaches. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preparation\n",
    "In order to implement the algorithm, we first import the required Python modules: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                          # Used for numerical computations\n",
    "import matplotlib.pyplot as plt             # Plotting library  \n",
    "\n",
    "# This is to set the size of the plots in the notebook\n",
    "plt.rcParams['figure.figsize'] = [9, 9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Toy Dataset\n",
    "Next, we will create a toy dataset. It will contain samples which are drawn from 3 normal distributions, where each distribution represents a class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_c = 3 # Number of clusters\n",
    "colours = [[255, 170, 170], [170, 255, 170], [170, 170, 255]]\n",
    "\n",
    "# Generate the samples (3 clusters), we set a fixed seed make the results reproducable\n",
    "np.random.seed(0)\n",
    "c1_samples = np.clip([(20, 20) + np.random.randn(2)*15 for i in range(33)], 0, 100)\n",
    "c2_samples = np.clip([(70, 30) + np.random.randn(2)*15 for i in range(33)], 0, 100)\n",
    "c3_samples = np.clip([(30, 70) + np.random.randn(2)*15 for i in range(33)], 0, 100)\n",
    "\n",
    "# Plot the samples, colored by class\n",
    "plt.scatter(*zip(*c1_samples), c='red', marker='o', label='Samples of class 1')\n",
    "plt.scatter(*zip(*c2_samples), c='green', marker='o', label='Samples of class 2')\n",
    "plt.scatter(*zip(*c3_samples), c='blue', marker='o', label='Samples of class 3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stack all data samples to one matrix and append the class index as additional column. Each row in the resulting matrix contains one the x and y coordinates and the class index of one sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_array = np.hstack((np.array(c1_samples), 1 * np.ones((len(c1_samples), 1))))\n",
    "c2_array = np.hstack((np.array(c2_samples), 2 * np.ones((len(c2_samples), 1))))\n",
    "c3_array = np.hstack((np.array(c3_samples), 3 * np.ones((len(c3_samples), 1))))\n",
    "\n",
    "all_samples = np.vstack((c1_array, c2_array, c3_array))\n",
    "print('Shape of stacked sample matrix:', all_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDE-Classification\n",
    "In Kernel Density Estimation, the idea is to fix the control volume $V$ and count the number of samples per class, that fall into the volume. Usually the counting is done in a smoothed way via a kernel function. This function acts as a measurement of contribution of each sample to the region of interest. The closer a sample $x_i$ is to the feature $x$ which is to be classified, the higher the contribution. Here we use a multivariate normal distribution\n",
    "\n",
    "$k(x_i\\mid x,\\sigma^2 ) = \\dfrac{1}{\\sqrt{(2\\pi)^n det(\\Sigma)}}\\cdot exp(-\\dfrac{1}{2}(x_i-x)^T\\Sigma^{-1}(x_i-x))$\n",
    "\n",
    "where $\\sigma$ defines the band with and thus corresponds to the size of $V$ and $n$ is the number of features (here 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, x_i, sigma):\n",
    "    Sigma = np.eye(2)*sigma # we do not consider covariances here\n",
    "    inv = np.linalg.inv(Sigma)\n",
    "    det = np.linalg.det(Sigma)\n",
    "    dif = x_i-x[:,:2]\n",
    "    return 1 / (np.sqrt((2*np.pi)**len(x)*det)) * np.exp(-0.5* np.sum((dif@inv)*(dif), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict the class of a new feature $x$ we compute the conditional probability\n",
    "\n",
    "$p(x\\mid C=L^k) = \\dfrac{1}{N_k}\\cdot \\sum\\limits_{i=1}^{N_k}k(x_i|x,\\sigma^2 )$\n",
    "\n",
    "for all classes and return the most probable class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_kde(x, data, sigma):\n",
    "    weights = gaussian_kernel(data, x, sigma)\n",
    "    classes = list(set(data[:,2]))\n",
    "    sums = [np.sum(weights[data[:,2]==c]) for c in classes]\n",
    "    counts = [np.sum(data[:,2]==c) for c in classes]\n",
    "    return np.argmax(np.array(sums)/np.array(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "To visualize the decision boundaries we predict every feature of the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_feature_space_kde(data, sigma):\n",
    "    label_map = np.zeros((100, 100, 3), dtype=np.ubyte)\n",
    "    for x in range(100):\n",
    "        for y in range(100):\n",
    "            label = predict_kde(np.array((x, y), dtype=np.float), data, sigma)\n",
    "            label_map[y, x] = colours[label]\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the algorithm for different values for $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_sigma_1 = predict_feature_space_kde(all_samples, 1)\n",
    "label_map_sigma_20 = predict_feature_space_kde(all_samples, 20)\n",
    "label_map_sigma_50 = predict_feature_space_kde(all_samples, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label_map_sigma_1)\n",
    "plt.scatter(*zip(*c1_samples), c='red', marker='o', label='Samples of class 1')\n",
    "plt.scatter(*zip(*c2_samples), c='green', marker='o', label='Samples of class 2')\n",
    "plt.scatter(*zip(*c3_samples), c='blue', marker='o', label='Samples of class 3')\n",
    "plt.title('KDE with s = 1'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label_map_sigma_20)\n",
    "plt.scatter(*zip(*c1_samples), c='red', marker='o', label='Samples of class 1')\n",
    "plt.scatter(*zip(*c2_samples), c='green', marker='o', label='Samples of class 2')\n",
    "plt.scatter(*zip(*c3_samples), c='blue', marker='o', label='Samples of class 3')\n",
    "plt.title('KDE with s = 20'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label_map_sigma_50)\n",
    "plt.scatter(*zip(*c1_samples), c='red', marker='o', label='Samples of class 1')\n",
    "plt.scatter(*zip(*c2_samples), c='green', marker='o', label='Samples of class 2')\n",
    "plt.scatter(*zip(*c3_samples), c='blue', marker='o', label='Samples of class 3')\n",
    "plt.title('KDE with s = 50'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can nicely see how the resulting boundaries get smoother as we increase the band with $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN-Classification\n",
    "The idea behind the K Nearest Neighbour classification is to freeze the number of neighbours per class and measure the minimum size of the required control volume that is required to include all of them. However, here we use the direct classification, where we simply take the K nearest neighbours and assign the most frequent class among these samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_knn(x, data, K):\n",
    "    num_samples = data.shape[0]\n",
    "    distances = np.array([np.sqrt(np.sum((x-data[i, :2])**2)) for i in range(num_samples)])\n",
    "    K_nearest_neighbours = np.argsort(distances)[:K]\n",
    "    nearest_classes = data[K_nearest_neighbours, 2].astype(np.int)\n",
    "    return np.argmax(np.bincount(nearest_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Again, we classify each feature in the feature space to visualize the decision boundaries.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_feature_space_knn(data, K):\n",
    "    label_map = np.zeros((100, 100, 3), dtype=np.ubyte)\n",
    "    for x in range(100):\n",
    "        for y in range(100):\n",
    "            label = predict_knn(np.array((x, y), dtype=np.float), data, K)\n",
    "            label_map[y, x] = colours[label-1]\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and evaluate the method. This time with different values for the number of neighbours $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_K1 = predict_feature_space_knn(all_samples, 1)\n",
    "label_map_K3 = predict_feature_space_knn(all_samples, 3)\n",
    "label_map_K15 = predict_feature_space_knn(all_samples, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label_map_K1)\n",
    "plt.scatter(*zip(*c1_samples), c='red', marker='o', label='Samples of class 1')\n",
    "plt.scatter(*zip(*c2_samples), c='green', marker='o', label='Samples of class 2')\n",
    "plt.scatter(*zip(*c3_samples), c='blue', marker='o', label='Samples of class 3')\n",
    "plt.title('KNN with K = 1'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label_map_K3)\n",
    "plt.scatter(*zip(*c1_samples), c='red', marker='o', label='Samples of class 1')\n",
    "plt.scatter(*zip(*c2_samples), c='green', marker='o', label='Samples of class 2')\n",
    "plt.scatter(*zip(*c3_samples), c='blue', marker='o', label='Samples of class 3')\n",
    "plt.title('KNN with K = 3'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label_map_K15)\n",
    "plt.scatter(*zip(*c1_samples), c='red', marker='o', label='Samples of class 1')\n",
    "plt.scatter(*zip(*c2_samples), c='green', marker='o', label='Samples of class 2')\n",
    "plt.scatter(*zip(*c3_samples), c='blue', marker='o', label='Samples of class 3')\n",
    "plt.title('KNN with K = 15'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "In this example, both  are suitable for classification if the hyper-parameters are set properly. Since each method only has one hyper-parameter they are easy tunable and they do not require any training. Again, a drawback is that all data has to be ready in memory and the implementation can have a major impact on the runtime!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
