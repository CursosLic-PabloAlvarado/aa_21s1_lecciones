{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy learning\n",
    "\n",
    "Based on: Gili Karni's\n",
    "https://medium.com/swlh/policy-gradient-reinforcement-learning-with-keras-57ca6ed32555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "# Some times this helps improving speed\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config ##\n",
    "ENV=\"CartPole-v1\"\n",
    "RANDOM_SEED=1\n",
    "N_EPISODES=1000\n",
    "FILENAME='cartpole.h5'\n",
    "\n",
    "\n",
    "# random seed (reproduciblity)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# set the env\n",
    "env=gym.make(ENV) # env to import\n",
    "env.seed(RANDOM_SEED)\n",
    "env.reset() # reset to env "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the REINFORCE agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \n",
    "    def __init__(self, env, path=None):\n",
    "        self.env=env #import env\n",
    "        self.state_shape=env.observation_space.shape # the state space\n",
    "        self.action_shape=env.action_space.n # the action space\n",
    "        self.gamma=0.99 # decay rate of past observations\n",
    "        self.alpha=1e-4 # learning rate in the policy gradient\n",
    "        self.learning_rate=0.01 # learning rate in deep learning\n",
    "\n",
    "        if not path:\n",
    "          self.model=self._create_model() #build model\n",
    "        else:\n",
    "          self.model=self.load_model(path) #import model\n",
    "\n",
    "        # record observations\n",
    "        self.states=[]\n",
    "        self.gradients=[] \n",
    "        self.rewards=[]\n",
    "        self.probs=[]\n",
    "        self.discounted_rewards=[]\n",
    "        self.total_rewards=[]\n",
    "  \n",
    "\n",
    "    def _create_model(self):\n",
    "        ''' builds the model using keras'''\n",
    "        model=Sequential()\n",
    "\n",
    "        # input shape is of observations\n",
    "        model.add(Dense(24, input_shape=self.state_shape, activation=\"relu\"))\n",
    "        # model.add(Dropout(0.05))\n",
    "        model.add(Dense(24, input_shape=self.state_shape, activation=\"relu\"))\n",
    "        # model.add(Dropout(0.05))\n",
    "        model.add(Dense(12, activation=\"relu\"))\n",
    "        # model.add(Dropout(0.5))    \n",
    "\n",
    "        # output shape is according to the number of action\n",
    "        # The softmax function outputs a probability distribution over the actions\n",
    "        model.add(Dense(self.action_shape, activation=\"softmax\")) \n",
    "        model.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def hot_encode_action(self, action):\n",
    "        '''encoding the actions into a binary list'''\n",
    "\n",
    "        action_encoded=np.zeros(self.action_shape, np.float32)\n",
    "        action_encoded[action]=1\n",
    "\n",
    "        return action_encoded\n",
    "  \n",
    "\n",
    "    def remember(self, state, action, action_prob, reward):\n",
    "        '''stores observations'''\n",
    "        encoded_action=self.hot_encode_action(action)\n",
    "        self.gradients.append(encoded_action-action_prob)\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "        self.probs.append(action_prob)\n",
    "\n",
    "  \n",
    "    def get_action(self, state):\n",
    "        '''samples the next action based on the policy probabilty distribution \n",
    "          of the actions'''\n",
    "\n",
    "        # transform state\n",
    "        state=state.reshape([1, state.shape[0]])\n",
    "        # get action probably\n",
    "        action_probability_distribution=self.model.predict(state).flatten()\n",
    "        # norm action probability distribution\n",
    "        action_probability_distribution/=np.sum(action_probability_distribution)\n",
    "\n",
    "        # sample action\n",
    "        action=np.random.choice(self.action_shape,1,\n",
    "                                p=action_probability_distribution)[0]\n",
    "\n",
    "        return action, action_probability_distribution\n",
    "\n",
    "\n",
    "    def get_discounted_rewards(self, rewards): \n",
    "        '''Use gamma to calculate the total reward discounting for rewards\n",
    "        Following - \\gamma ^ t * Gt'''\n",
    "\n",
    "        discounted_rewards=[]\n",
    "        cumulative_total_return=0\n",
    "        # iterate the rewards backwards and and calc the total return \n",
    "        for reward in rewards[::-1]:      \n",
    "          cumulative_total_return=(cumulative_total_return*self.gamma)+reward\n",
    "          discounted_rewards.insert(0, cumulative_total_return)\n",
    "\n",
    "        # normalize discounted rewards\n",
    "        mean_rewards=np.mean(discounted_rewards)\n",
    "        std_rewards=np.std(discounted_rewards)\n",
    "        norm_discounted_rewards=(discounted_rewards-\n",
    "                                 mean_rewards)/(std_rewards+1e-7) # avoiding zero div\n",
    "\n",
    "        return norm_discounted_rewards\n",
    "\n",
    "    \n",
    "    def update_policy(self):\n",
    "        '''Updates the policy network using the NN model.\n",
    "        This function is used after the MC sampling is done - following\n",
    "        \\delta \\theta = \\alpha * gradient + log pi'''\n",
    "\n",
    "        # get X\n",
    "        states=np.vstack(self.states)\n",
    "\n",
    "        # get Y\n",
    "        gradients=np.vstack(self.gradients)\n",
    "        rewards=np.vstack(self.rewards)\n",
    "        discounted_rewards=self.get_discounted_rewards(rewards)\n",
    "        gradients*=discounted_rewards\n",
    "        gradients=self.alpha*np.vstack([gradients])+self.probs\n",
    "\n",
    "        history=self.model.train_on_batch(states, gradients)\n",
    "\n",
    "        self.states, self.probs, self.gradients, self.rewards=[], [], [], []\n",
    "\n",
    "        return history\n",
    "\n",
    "    \n",
    "    def train(self, episodes, rollout_n=1, render_n=50):\n",
    "        '''train the model\n",
    "            episodes - number of training iterations \n",
    "            rollout_n- number of episodes between policy update\n",
    "            render_n - number of episodes between env rendering ''' \n",
    "\n",
    "        env=self.env\n",
    "        total_rewards=np.zeros(episodes)\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            clear_output(wait=True)\n",
    "            print(\"Episode:\",episode)\n",
    "            # each episode is a new game env\n",
    "            state=env.reset()\n",
    "            done=False          \n",
    "            episode_reward=0 #record episode reward\n",
    "\n",
    "            while not done:\n",
    "                # play an action and record the game state & reward per episode\n",
    "                action, prob=self.get_action(state)\n",
    "                next_state, reward, done, _=env.step(action)\n",
    "                self.remember(state, action, prob, reward)\n",
    "                state=next_state\n",
    "                episode_reward+=reward\n",
    "\n",
    "                if episode%render_n==0: ## render env to visualize.\n",
    "                  env.render()\n",
    "\n",
    "                if done:\n",
    "                  # update policy \n",
    "                  if episode%rollout_n==0:\n",
    "                    history=self.update_policy()\n",
    "                    print(\"  loss: \",history)\n",
    "\n",
    "            total_rewards[episode]=episode_reward\n",
    "\n",
    "        self.total_rewards=total_rewards\n",
    "\n",
    "        \n",
    "    def save_model(self,path):\n",
    "        '''saves the model // do after training'''\n",
    "        self.model.save(path)\n",
    "  \n",
    "\n",
    "    def load_model(self, filename):\n",
    "        '''loads a trained model from path'''\n",
    "        \n",
    "        if path.exists(filename):   \n",
    "    \n",
    "            print(\"Reading file '{0}'\".format(filename))\n",
    "            return load_model(filename)  #load_model from keras\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return self._create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create agent and train the model\n",
    "agent=REINFORCE(env,path=FILENAME)\n",
    "\n",
    "agent.train(episodes=N_EPISODES, rollout_n=3, render_n=50)\n",
    "\n",
    "agent.save_model(FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('REINFORCE Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average reward (Episode length)')\n",
    "plt.plot(agent.total_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:\n",
    "\n",
    "## The reward should go up.  If it doesn't, fix it.\n",
    "## Using the learned model, show episodes with the render method using it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
